# AgentState — the shared state TypedDict that flows through every graph node.
#
# MERGE BEHAVIOR (critical architectural rule):
#   - `messages` uses add_messages reducer → new messages APPEND to checkpoint history
#   - All other fields:
#       * If included in initial_input → REPLACES the checkpoint value for that turn
#       * If NOT in initial_input → checkpoint value PERSISTS unchanged
#
# Field classification (enforced in Phase 2.6 initial state construction):
#
#   PERSISTENT — NEVER include in initial_input:
#     conversation_docs   ← name→UUID registry built over multiple turns
#
#   TRANSIENT — ALWAYS include in initial_input (reset each turn):
#     Everything else — stale values from the previous turn must not bleed through
#
# Nodes return partial dicts with only the keys they write; LangGraph merges
# the update into the checkpoint via the reducer (messages) or plain replace (all others).

from typing import Annotated, Optional, TypedDict

# Sentinel sent as Command(resume=...) when the user cancels a PalAssist prompt.
# LangGraph 1.0.x crashes on Command(resume=None) due to an unbound variable bug
# in _loop.py (_first method). Using a string sentinel bypasses this while keeping
# cancel semantics identical — nodes check `resume_val in (None, CANCEL_SENTINEL)`.
CANCEL_SENTINEL = "__cancel__"

from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages


# ---------------------------------------------------------------------------
# NODE_STATUS_MAP — single source of truth for PalReasoning status messages.
# Each graph node name maps to the human-readable text shown in the UI
# while that node is executing.  Add a new node → add an entry here.
# ---------------------------------------------------------------------------

NODE_STATUS_MAP: dict[str, str] = {
    "intent_resolver": "Clarifying intent...",
    "doc_resolver": "Finding documents...",
    "validate_inputs": "Validating request...",
    "summarize": "Summarizing documents...",
    "inquire": "Researching your question...",
    "compare": "Comparing documents...",
    "audit": "Auditing against regulations...",
    "format_response": "Formatting response...",
}


# ---------------------------------------------------------------------------
# AgentState
# ---------------------------------------------------------------------------

class AgentState(TypedDict, total=False):
    """
    Shared state flowing through every LangGraph node.

    total=False makes all keys optional in the TypedDict sense — nodes return
    partial dicts containing only the keys they update, and LangGraph merges
    those updates into the checkpoint.  The exception is `messages`, which has
    an add_messages reducer that appends rather than replaces.

    Nodes read fields with state.get("key", default) so the first-message
    case (empty checkpoint) is handled gracefully.
    """

    # ── Core conversation fields ────────────────────────────────────────────

    # add_messages reducer: new messages APPEND; never replaces existing history
    messages: Annotated[list[AnyMessage], add_messages]
    thread_id: str   # = conversation.id; set once per run
    user_id: str     # for Supabase RLS queries; set once per run

    # ── Action routing fields ───────────────────────────────────────────────

    # "summarize" | "inquire" | "compare" | "audit"
    # Set by intent_resolver (or passed directly when user tags an action)
    action: str

    # OR of: frontend flag + Python keyword scan + LLM detection
    enable_web_search: bool

    # Generated by action nodes (NOT intent_resolver)
    web_search_query: Optional[str]

    # Tavily results (populated in future iteration)
    web_search_results: Optional[list]

    # ── Document resolution fields ──────────────────────────────────────────

    # UUIDs extracted from TipTap @mention nodes — set once per run from input
    explicit_doc_ids: list[str]

    # Raw TipTap JSON for Python pre-processor name extraction
    tiptap_json: dict

    # Final resolved document UUIDs that action nodes operate on
    resolved_doc_ids: list[str]

    # "explicit" | "inferred" | "fuzzy_match" — written by doc_resolver
    inference_source: str

    # UUID of a tagged document set (if the user @mentioned a set)
    set_id: Optional[str]

    # PERSISTENT: { "Document Title": "uuid" } — built across turns.
    # NEVER include in initial_input — including {} would wipe the registry.
    conversation_docs: dict[str, str]

    # True when the message contains implicit doc references (no @mention)
    has_implicit_refs: bool

    # UUIDs resolved by the LLM from implicit references
    inferred_doc_ids: list[str]

    # Names the LLM flagged as unresolvable → fed to rapidfuzz fallback
    unresolved_names: list[str]

    # "high" | "medium" | "low" — written by doc_resolver
    inference_confidence: str

    # LLM's reasoning for its doc resolution decision (LangSmith debugging only)
    inference_reasoning: Optional[str]

    # Candidate UUIDs surfaced for PalAssist medium-confidence choice prompt
    suggested_doc_ids: list[str]

    # ── Query intelligence fields (set by doc_resolver, read by action nodes) ─

    # Plain text from TipTap walk with all @mentions stripped.
    # Computed once in doc_resolver; replaces broken per-node regex in action nodes.
    # TRANSIENT — reset each turn in chat.py initial_state.
    clean_query: str

    # { uuid: title } for every resolved document — built from TipTap labels +
    # conversation_docs registry in doc_resolver. Zero extra DB calls.
    # Used by rewrite_query() and action nodes for citation enrichment.
    # TRANSIENT — reset each turn in chat.py initial_state.
    resolved_doc_titles: dict  # dict[str, str]

    # ── Retrieval result fields (set by action nodes) ───────────────────────

    # RAG chunks (empty list for stub actions until Iterations 2-3)
    retrieved_chunks: list

    # "high" | "medium" | "low" — written by action node
    retrieval_confidence: str

    # 0.0-1.0 average cosine similarity across retrieved chunks
    confidence_score: float

    # ── Response fields ─────────────────────────────────────────────────────

    # Final AI response text — written by action/format_response node
    response: str

    # Citation objects — { doc_id, title, page, excerpt }
    citations: list[dict]

    # Accumulated token usage for this run
    tokens_used: int

    # Accumulated USD cost for this run
    cost_usd: float

    # ── User context (injected from profiles table at run start) ────────────
    # Read-only inside nodes — injected once by chat.py before graph.invoke()
    user_industry: Optional[str]
    user_location: Optional[str]

    # ── Long conversation management (Phase 5) ───────────────────────────────
    # Rolling summary of messages that have been windowed out.
    # Persists across turns; never included in initial_input.
    conversation_summary: Optional[str]
